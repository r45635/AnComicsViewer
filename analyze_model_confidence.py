#!/usr/bin/env python3
"""
Analyse des seuils de confiance pour comprendre la diff√©rence de d√©tection
"""

import sys
from pathlib import Path
from ultralytics import YOLO
import matplotlib.pyplot as plt
import numpy as np
import torch

# Fix pour PyTorch 2.8.0 - charger avec weights_only=False
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

def load_model_safe(model_path):
    """Charge un mod√®le YOLO en g√©rant les probl√®mes de compatibilit√©."""
    try:
        return YOLO(model_path)
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur chargement mod√®le: {e}")
        return None

def analyze_confidence_thresholds():
    """Analyse la d√©tection avec diff√©rents seuils de confiance."""
    
    print("üîç ANALYSE DES SEUILS DE CONFIANCE")
    print("=" * 50)
    
    # Mod√®les
    old_model_path = "runs/detect/overfit_small/weights/best.pt"
    new_model_path = "runs/detect/mixed_golden_tintin/weights/best.pt"
    
    if not Path(old_model_path).exists():
        print(f"‚ùå Ancien mod√®le non trouv√©: {old_model_path}")
        return
        
    if not Path(new_model_path).exists():
        print(f"‚ùå Nouveau mod√®le non trouv√©: {new_model_path}")
        return
    
    print("‚úÖ Chargement des mod√®les...")
    old_model = load_model_safe(old_model_path)
    new_model = load_model_safe(new_model_path)
    
    if old_model is None or new_model is None:
        print("‚ùå Impossible de charger les mod√®les")
        return
    
    # Images de test
    test_images = [
        ("dataset/images/train/p0003.png", "Golden City"),
        ("dataset/images/train/tintin_p0001.png", "Tintin")
    ]
    
    # Seuils √† tester
    thresholds = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5]
    
    for img_path, style in test_images:
        if not Path(img_path).exists():
            print(f"‚ö†Ô∏è  Image non trouv√©e: {img_path}")
            continue
            
        print(f"\nüì∏ {style} - {Path(img_path).name}")
        print("-" * 40)
        print("Seuil  | Ancien | Nouveau | Diff")
        print("-" * 40)
        
        for threshold in thresholds:
            # Ancien mod√®le
            old_results = old_model.predict(img_path, conf=threshold, verbose=False)
            old_panels = len(old_results[0].boxes) if old_results[0].boxes is not None else 0
            
            # Nouveau mod√®le  
            new_results = new_model.predict(img_path, conf=threshold, verbose=False)
            new_panels = len(new_results[0].boxes) if new_results[0].boxes is not None else 0
            
            diff = new_panels - old_panels
            diff_str = f"+{diff}" if diff > 0 else str(diff)
            
            print(f"{threshold:5.2f}  |   {old_panels:2d}   |    {new_panels:2d}    |  {diff_str:>3}")

def analyze_detection_scores():
    """Analyse les scores de confiance des d√©tections."""
    
    print("\nüéØ ANALYSE DES SCORES DE CONFIANCE")
    print("=" * 50)
    
    # Mod√®les
    old_model_path = "runs/detect/overfit_small/weights/best.pt"
    new_model_path = "runs/detect/mixed_golden_tintin/weights/best.pt"
    
    old_model = load_model_safe(old_model_path)
    new_model = load_model_safe(new_model_path)
    
    if old_model is None or new_model is None:
        print("‚ùå Impossible de charger les mod√®les")
        return
    
    # Image de test
    test_image = "dataset/images/train/p0003.png"
    if not Path(test_image).exists():
        print(f"‚ùå Image test non trouv√©e: {test_image}")
        return
    
    print(f"üì∏ Analyse d√©taill√©e: {Path(test_image).name}")
    
    # Pr√©dictions avec seuil tr√®s bas
    old_results = old_model.predict(test_image, conf=0.01, verbose=False)
    new_results = new_model.predict(test_image, conf=0.01, verbose=False)
    
    print("\n=== ANCIEN MOD√àLE ===")
    if old_results[0].boxes is not None:
        confidences = old_results[0].boxes.conf.cpu().numpy()
        classes = old_results[0].boxes.cls.cpu().numpy()
        
        print(f"D√©tections totales: {len(confidences)}")
        for i, (conf, cls) in enumerate(zip(confidences, classes)):
            class_name = "panel" if cls == 0 else "panel_inset"
            print(f"  {i+1:2d}. {class_name:11} - conf: {conf:.3f}")
        
        print(f"Score moyen: {np.mean(confidences):.3f}")
        print(f"Score m√©dian: {np.median(confidences):.3f}")
        print(f"Score min: {np.min(confidences):.3f}")
        print(f"Score max: {np.max(confidences):.3f}")
    
    print("\n=== NOUVEAU MOD√àLE ===")
    if new_results[0].boxes is not None:
        confidences = new_results[0].boxes.conf.cpu().numpy()
        classes = new_results[0].boxes.cls.cpu().numpy()
        
        print(f"D√©tections totales: {len(confidences)}")
        for i, (conf, cls) in enumerate(zip(confidences, classes)):
            class_name = "panel" if cls == 0 else "panel_inset"
            print(f"  {i+1:2d}. {class_name:11} - conf: {conf:.3f}")
            
        print(f"Score moyen: {np.mean(confidences):.3f}")
        print(f"Score m√©dian: {np.median(confidences):.3f}")
        print(f"Score min: {np.min(confidences):.3f}")
        print(f"Score max: {np.max(confidences):.3f}")

if __name__ == "__main__":
    analyze_confidence_thresholds()
    analyze_detection_scores()
