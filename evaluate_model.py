#!/usr/bin/env python3
"""
Ã‰valuation personnalisÃ©e du modÃ¨le YOLO entraÃ®nÃ©
"""

import os
import torch
from pathlib import Path
from ultralytics import YOLO
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

def evaluate_model():
    """Ã‰value le modÃ¨le sur le dataset de validation"""
    print("ðŸ” Ã‰valuation personnalisÃ©e du modÃ¨le...")
    print("=" * 50)

    # Trouver le modÃ¨le
    model_dirs = sorted(Path("./runs/detect").glob("ancomics_final_optimized*"))
    if not model_dirs:
        print("âŒ Aucun modÃ¨le trouvÃ©")
        return

    model_dir = model_dirs[-1]  # Dernier modÃ¨le
    model_path = model_dir / "weights" / "best.pt"

    if not model_path.exists():
        model_path = model_dir / "weights" / "last.pt"

    print(f"ðŸ“ ModÃ¨le: {model_path}")

    # Charger le modÃ¨le
    model = YOLO(str(model_path))

    # Chemin des donnÃ©es de validation
    val_images = Path("./dataset/images/val")
    val_labels = Path("./dataset/labels/val")

    if not val_images.exists() or not val_labels.exists():
        print("âŒ DonnÃ©es de validation introuvables")
        return

    # Collecter les images de validation
    image_files = list(val_images.glob("*.png")) + list(val_images.glob("*.jpg"))
    print(f"ðŸ–¼ï¸  Images de validation: {len(image_files)}")

    # Statistiques
    total_predictions = 0
    total_ground_truth = 0
    class_predictions = {0: 0, 1: 0}  # panel, balloon
    class_ground_truth = {0: 0, 1: 0}

    print("\nðŸŽ¯ Analyse des prÃ©dictions...")

    for img_path in tqdm(image_files, desc="Traitement"):
        # Charger l'image
        img = Image.open(img_path)

        # PrÃ©diction
        results = model(img, conf=0.25, iou=0.6, verbose=False)

        # Compter les prÃ©dictions
        if results and len(results) > 0:
            result = results[0]
            if result.boxes is not None:
                pred_classes = result.boxes.cls.cpu().numpy().astype(int)
                for cls in pred_classes:
                    if cls in [0, 1]:
                        class_predictions[cls] += 1
                        total_predictions += 1

        # Charger les ground truth
        label_file = val_labels / f"{img_path.stem}.txt"
        if label_file.exists():
            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        cls = int(parts[0])
                        if cls in [0, 1]:
                            class_ground_truth[cls] += 1
                            total_ground_truth += 1

    # RÃ©sultats
    print("\nðŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION:")
    print(f"   Ground Truth - Panels: {class_ground_truth[0]}, Balloons: {class_ground_truth[1]}")
    print(f"   PrÃ©dictions - Panels: {class_predictions[0]}, Balloons: {class_predictions[1]}")
    print(f"   Total Ground Truth: {total_ground_truth}")
    print(f"   Total PrÃ©dictions: {total_predictions}")

    # Calcul de mÃ©triques simples
    if total_ground_truth > 0:
        recall = total_predictions / total_ground_truth
        print(f"   Recall approximatif: {recall:.3f}")

    # VÃ©rifier que le modÃ¨le dÃ©tecte quelque chose
    if total_predictions > 0:
        print("âœ… Le modÃ¨le dÃ©tecte des objets!")
        print("ðŸŽ‰ EntraÃ®nement rÃ©ussi!")
    else:
        print("âš ï¸  Le modÃ¨le ne dÃ©tecte aucun objet")

    return total_predictions > 0

if __name__ == "__main__":
    evaluate_model()
